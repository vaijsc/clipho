clip_benchmark eval --model ViT-B-16-SigLIP-384 --pretrained webli --dataset "multilingual_mscoco_captions" --dataset_root data/eval_data/mscoco/ --language "en" --batch_size 1024 --recall_k 1 5 10 --output tmp/result_mscoco_ViT-B-16-SigLIP-384-webli_clip_benchmark.json
clip_benchmark eval --model ViT-B-16-SigLIP-384 --pretrained webli --dataset "flickr30k-200" --dataset_root data/eval_data/flickr30k/ --language "eng_Latn" --batch_size 1024 --recall_k 1 5 10 --output tmp/result_flickr30k_ViT-B-16-SigLIP-384-webli_clip_benchmark.json
clip_benchmark eval --model ViT-B-16-SigLIP-384 --pretrained webli --dataset "crossmodal3600" --dataset_root data/eval_data/crossmodal3600/ --language "en" --batch_size 1024 --recall_k 1 5 10 --output tmp/result_crossmodal3600_ViT-B-16-SigLIP-384-webli_clip_benchmark.json
clip_benchmark eval --model ViT-B-16-SigLIP-384 --pretrained webli --dataset "xtd200" --dataset_root data/eval_data/xtd10/ --language "eng_Latn" --batch_size 1024 --recall_k 1 5 10 --output tmp/result_xtd200_ViT-B-16-SigLIP-384-webli_clip_benchmark.json